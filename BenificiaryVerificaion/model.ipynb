{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`This is the enhanced version of the code.`**\n",
    "### **`Includes`**\n",
    "- *Model Checkpoints && Callbacks*\n",
    "- *Controlling Overfitting and Underfitting with Dropout, Regularization, BatchNorm etc.*\n",
    "- *Tensorboard Visuaization*\n",
    "- *Data Visualization*\n",
    "- *Model Plotting with keras.utils*\n",
    "- *HyperParameter Tuning with Keras Tuner*\n",
    "- *Saving and Loading Model*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https: // github.com/tensorflow/docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pathlib\n",
    "import shutil\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = pathlib.Path(tempfile.mkdtemp())/\"tensorboard_logs\"\n",
    "shutil.rmtree(logdir, ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paths to the training and testing directories\n",
    "train_dir = 'path/to/training/directory'\n",
    "test_dir = 'path/to/testing/directory'\n",
    "\n",
    "# Set the batch size and image size\n",
    "batch_size = 32\n",
    "img_size = (224, 224)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **NEED TO SEE IF WE NEED THIS CODE**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N_VALIDATION = int(1e3)\n",
    "N_TRAIN = int(1e4)\n",
    "BUFFER_SIZE = int(1e4)\n",
    "BATCH_SIZE = 64\n",
    "STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE\n",
    "\n",
    "# NOTE: Use the Dataset.cache method to ensure that the loader doesn't\n",
    "#       need to re-read the data from the file on each epoch.\n",
    "\n",
    "validate_ds = packed_ds.take(N_VALIDATION).cache()\n",
    "train_ds = packed_ds.skip(N_VALIDATION).take(N_TRAIN).cache()\n",
    "\n",
    "\n",
    "# Use the Dataset.batch method to create batches of an appropriate size for training.\n",
    "# Use Dataset.shuffle and Dataset.repeat on the training set.\n",
    "\n",
    "validate_ds = validate_ds.batch(BATCH_SIZE)\n",
    "train_ds = train_ds.shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **FIXME HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data augmentation to avoid overfitting\n",
    "# FIXME: Need to replace this `ImageDataGenerator` with `tf.keras.utils.image_dataset_from_directory` as the former is deprocated!\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data generator for training and testing\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained VGG16 model\n",
    "base_model = VGG16(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(img_size[0], img_size[1], 3)\n",
    ")\n",
    "\n",
    "# Freeze the layers in the pre-trained model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a few dense layers for classification\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    0.001,\n",
    "    decay_steps=STEPS_PER_EPOCH*1000,\n",
    "    decay_rate=1,\n",
    "    staircase=False\n",
    ")\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "model.compile(\n",
    "\n",
    "    # NOTE: Uncomment the optimizer with the schedular.\n",
    "    # optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "    optimizer='adam',\n",
    "\n",
    "    # NOTE: Change the loss function from Categorical Cross Entropy to SCCE.\n",
    "    loss='categorical_crossentropy',\n",
    "    # loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Custom Code**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Add callbacks to facilitate easy model recovery!\n",
    "\n",
    "callbacks = [\n",
    "    # The saved model name will include the current epoch.\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        # Path where to save the model\n",
    "        filepath=\"mymodel_{epoch}\",\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # ! Only save a model if `val_loss` has improved, this can be problematic!.\n",
    "        save_best_only=True,\n",
    "\n",
    "        # the `val_loss` score has improved.\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=1,\n",
    "    )\n",
    "]\n",
    "model.fit(\n",
    "    x_train, y_train, epochs=2, batch_size=64, callbacks=callbacks, validation_split=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=test_generator\n",
    "    # batch_size=64,\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# Save the model\n",
    "model.save('injury_damage_disaster_classifier.h5')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
